{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Preparation",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "55fe6fe6-2d27-4d89-a29f-3913f21d5d24",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Import",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00001-28237605-f2d6-4ee8-885c-745f226f828d",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "64523fb1",
    "execution_start": 1651159701438,
    "execution_millis": 2477,
    "cell_id": "00002-7ee9a61a-a413-44ea-b98d-113fc5e92e6c",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 351.15625
   },
   "source": "# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport pathlib\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport PIL\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**AUTOTUNE**: \n\n* **map**: transform the dataset according to a function, in this case converts the images to float\n* **cache**: keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n* **prefetch**: for optimization porpuses ",
   "metadata": {
    "tags": [],
    "cell_id": "00003-c32f3752-2c56-4a93-be32-b02fc472e0cf",
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 198.390625
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Functions",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00004-b859e856-eee7-4ebb-8f28-b652a3c55bb1",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "af00a339",
    "execution_start": 1651161145099,
    "execution_millis": 1,
    "is_code_hidden": false,
    "cell_id": "00005-08519971-06f9-4dc7-8361-12b3a1059830",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117.15625
   },
   "source": "def convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### create dataset",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00006-c336ff3e-68db-4b8e-ae24-7cf5be507887",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6b0b01680810406eaf47fe95fc815c92",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 84.15625
   },
   "source": "\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47a033fe",
    "execution_start": 1651161062781,
    "execution_millis": 11,
    "is_code_hidden": false,
    "cell_id": "00007-6db50cb5-770c-4b10-960e-517e5f81d0c8",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1323.171875
   },
   "source": "def create_train_dataset(training_dir, batch_size = 32, greyscale = True):\n\n    color = 'grayscale'\n    if(not greyscale):\n        color = 'rgb'\n\n    # for fitting \n    train_ds = tf.keras.utils.image_dataset_from_directory(\n        training_dir,\n        seed=420,\n        image_size=(img_height, img_width),\n        batch_size= batch_size,\n        validation_split=0.2,\n        subset=\"training\",\n        color_mode=color,\n    )\n\n    val_ds = tf.keras.utils.image_dataset_from_directory(\n        training_dir,\n        seed=420,\n        image_size=(img_height, img_width),\n        batch_size= batch_size,\n        validation_split=0.2,\n        subset=\"validation\",\n        color_mode= color,\n    )\n\n\n\n\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n    ds_train = train_ds.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE)\n    ds_valid = val_ds.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE)\n   \n    class_names = train_ds.class_names\n\n    \n\n    return (ds_train, ds_valid, class_names)\n\n\n\ndef create_test_dataset( query_ds, gallery_ds,  batch_size = 32, greyscale = True):\n    color = 'grayscale'\n    if(not greyscale):\n        color = 'rgb'   \n\n    query_ds = tf.keras.utils.image_dataset_from_directory(\n        query_dir,\n        seed=420,\n        image_size=(img_height, img_width),\n        batch_size= batch_size,\n        color_mode= color,\n    )\n\n    gallery_ds = tf.keras.utils.image_dataset_from_directory(\n        gallery_dir,\n        seed=420,\n        image_size=(img_height, img_width),\n        batch_size= batch_size,\n        color_mode= color,\n    )\n\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n    ds_query = query_ds.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE)\n    ds_gallery = gallery_ds.map(convert_to_float).cache().prefetch(buffer_size=AUTOTUNE)\n\n    return (ds_query, ds_gallery)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### print 16 images",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00008-8f09480f-dd4e-4eec-a961-0e5994803dbc",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "99e123ce",
    "execution_start": 1651138763455,
    "execution_millis": 35,
    "is_code_hidden": false,
    "cell_id": "00009-01b48a1a-5cf5-48b7-a2d7-96ce7123ac0a",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 315.171875
   },
   "source": "def print_16_imgs(dataset):\n    imgs = np.concatenate([x for x, y in dataset], axis=0)\n    labels = np.concatenate([y for x, y in dataset], axis=0)\n\n    plt.figure(figsize=(10,10))\n    for i in range(16):\n        plt.subplot(4,4,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(imgs[i].astype('uint8'), cmap=plt.cm.binary)\n        plt.grid(False)\n        plt.title(class_names[labels[i]])\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### create model",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00010-beedc939-0f62-46e3-b253-cc8964926bb6",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "70c23211",
    "execution_start": 1651159712630,
    "execution_millis": 1,
    "is_code_hidden": false,
    "cell_id": "00011-19c40f52-2d5f-4533-ac24-9499468b47f5",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 855.171875
   },
   "source": "def create_model():\n\n    data_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, img_depth)),\n        layers.RandomRotation(0.1),\n        layers.RandomZoom(0.1),\n    ]\n    )\n\n\n    model = keras.Sequential([\n\n        data_augmentation,\n        layers.Rescaling(1./255), # only with rgb\n\n        layers.Conv2D(32, 3, padding='same', activation='relu' ),\n        layers.MaxPooling2D(),\n\n        layers.Conv2D(64, 3, padding='same', activation='relu'), #Â relu for hidden layers\n        layers.MaxPooling2D(),\n\n        layers.Dropout(0.3),\n        layers.Flatten(),\n        #layers.Dense(16),\n        layers.Dense(32, activation='relu'),\n       # layers.Dense(10, activation='softmax') # softmax for probablities \n\n\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss= tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=['accuracy'],\n    )\n\n    model.build((None, img_width, img_height, img_depth))\n\n\n    model.summary()\n    return model\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Parameters",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00012-7da00970-b692-4c0e-8a3e-6f840c6d2f43",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3ef534c2",
    "execution_start": 1651159717152,
    "execution_millis": 3,
    "is_code_hidden": false,
    "cell_id": "00013-8ffb6aa3-480d-4afa-82d1-cf9661294eec",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135.171875
   },
   "source": "img_height = 128\nimg_width = 128\nimg_depth = 3 # 1 if greyscale 3 if rgb\nn_epochs = 10",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Dataset and model",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00014-d95a13e8-6973-4db2-947a-07eb9a4f0557",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset directories",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00015-6f2e7e8b-359a-4e6f-8d2c-99ae79c5c6e9",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a5ce594f",
    "execution_start": 1651159719389,
    "execution_millis": 4,
    "cell_id": "00016-215430a9-703e-4233-b402-ddeefc4667e6",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 243.171875
   },
   "source": "# drive \ntraining_dir = pathlib.Path('/datasets/animali/animals_dataset_the_ostriches/animals_dataset/training')\nquery_dir = pathlib.Path('/datasets/animali/animals_dataset_the_ostriches/animals_dataset/validation2/query')\ngallery_dir = pathlib.Path('/datasets/animali/animals_dataset_the_ostriches/animals_dataset/validation2/gallery')\n#local\n#training_dir = pathlib.Path('/work/dataset/training')\n#query_dir = pathlib.Path('/work/dataset/validation/query')\n#gallery_dir = pathlib.Path('/work/dataset/validation/gallery')\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create dataset and fit the model",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00017-d2b4081c-2f8d-4e34-b20b-88b0a4f8eee2",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "346970512bb04259bddc1c3486d9d274",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e47a61b4",
    "execution_start": 1651160680530,
    "execution_millis": 59304,
    "owner_user_id": "29eb6caf-f5f9-44c1-9344-7ba4b9d5b779",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 3170.828125
   },
   "source": "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        training_dir,\n        target_size=(img_height, img_width),\n        batch_size=32,\n        #class_mode='binary'\n        )\n\n\nmodel = create_model()\n\nmodel.fit(train_generator,\n        steps_per_epoch=2000,\n        epochs=50,\n       # validation_data=validation_generator,\n        validation_steps=800)\n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 16607 images belonging to 10 classes.\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n sequential_2 (Sequential)   (None, 128, 128, 3)       0         \n                                                                 \n rescaling_1 (Rescaling)     (None, 128, 128, 3)       0         \n                                                                 \n conv2d_2 (Conv2D)           (None, 128, 128, 32)      896       \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 64, 64, 32)       0         \n 2D)                                                             \n                                                                 \n conv2d_3 (Conv2D)           (None, 64, 64, 64)        18496     \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 32, 32, 64)       0         \n 2D)                                                             \n                                                                 \n dropout_1 (Dropout)         (None, 32, 32, 64)        0         \n                                                                 \n flatten_1 (Flatten)         (None, 65536)             0         \n                                                                 \n dense_1 (Dense)             (None, 32)                2097184   \n                                                                 \n=================================================================\nTotal params: 2,116,576\nTrainable params: 2,116,576\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n      app.start()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/local/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/local/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n      ret = callback()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n      self.ctx_run(self.run)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n      user_expressions, allow_stdin,\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-8-9614c188e3f2>\", line 25, in <module>\n      validation_steps=800)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n      y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/losses.py\", line 1863, in sparse_categorical_crossentropy\n      y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/backend.py\", line 5203, in sparse_categorical_crossentropy\n      labels=target, logits=output)\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32,32] and labels shape [320]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_2059]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9614c188e3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m        \u001b[0;31m# validation_data=validation_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         validation_steps=800)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n      app.start()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/local/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/local/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n      ret = callback()\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n      self.ctx_run(self.run)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 358, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 538, in execute_request\n      user_expressions, allow_stdin,\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2958, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3003, in _run_cell\n      return runner(coro)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3229, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3524, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-8-9614c188e3f2>\", line 25, in <module>\n      validation_steps=800)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 860, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/training.py\", line 919, in compute_loss\n      y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/losses.py\", line 141, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/losses.py\", line 245, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/losses.py\", line 1863, in sparse_categorical_crossentropy\n      y_true, y_pred, from_logits=from_logits, axis=axis)\n    File \"/shared-libs/python3.7/py/lib/python3.7/site-packages/keras/backend.py\", line 5203, in sparse_categorical_crossentropy\n      labels=target, logits=output)\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32,32] and labels shape [320]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_2059]"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluation and prediction",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00019-b0c89c25-bb2d-47d3-99bb-3d1d70f2ccf7",
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Functions",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00020-76904016-0d66-4356-b176-fac2e8092b9f",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Get rank ",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00021-c5fa49f4-212a-4047-af07-46f0fd2e5df4",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b2c9751d",
    "execution_start": 1650897110603,
    "execution_millis": 4,
    "is_code_hidden": false,
    "cell_id": "00022-e71153b1-a4c1-4c89-963e-275c0e6196e1",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 729.15625
   },
   "source": "# given a prediction sorts according to the probabilities and returns a list of tuples (1, 0.2)\n# the tuple (1, 0.2) means that class 1 has 0.2 % of probability\ndef get_sorted_prediction(predictions):\n    a = np.argsort(predictions)[::-1] # sorted indexes \n    b = sorted(predictions)[::-1] # sorted probabilities \n    return zip(a,b)\n\n# this func is called for each query image return 10 element of the gallery (by their index)\ndef get_rank(query_pred, gallery_pred):\n\n    # sort the \n    query = list(get_sorted_prediction(query_pred)) # query\n\n    #gallery \n    gallery = {}\n    for i, pred in enumerate(gallery_pred):\n        gallery[i] = (list(get_sorted_prediction(pred)))\n\n    ranking = []\n\n    # iterate starting from the most probable class and find a match in the gallery \n    for class_number in query:\n        maxi = 0\n        max_idx = 99\n        for i, pred in gallery.items():\n            if class_number[0] == pred[0][0]:\n                if maxi < pred[0][1]:\n                    maxi = pred[0][1]\n                    max_idx = i\n        \n\n        if maxi > 0:\n            ranking.append(max_idx)\n\n    return ranking\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluate",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00023-5bd72235-499f-43b6-a6d9-0abed4c0da8e",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Ranking",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00025-d848a875-4b9d-4a13-9ff3-4ed4caf69e67",
    "deepnote_cell_type": "text-cell-h2"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# PARTO DA QUI",
   "metadata": {
    "cell_id": "45b5e2084512416dbcf94a8329051502",
    "tags": [],
    "is_collapsed": false,
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1abb4e72804749b7855ed266d50f850d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c69dbd28",
    "execution_start": 1651161424572,
    "execution_millis": 0,
    "owner_user_id": "4ffd0e12-c633-4f4a-8a0a-9942ba78ed45",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1215.171875
   },
   "source": "class Dataset(object):\n    def __init__(self, data_path):\n        #path of the dataset\n        self.data_path = data_path\n\n        #class list\n        self.data_classes = [directory for directory in os.listdir(data_path) if os.path.isdir(data_path+directory)]\n\n        # init lists and dictionary\n        self.images = []\n        self.labels = []\n        self.class_names = {}\n\n        # for each class and for each image save the image and the label in the lists \n        for c, c_name in enumerate(self.data_classes):\n            temp_path = os.path.join(self.data_path, c_name)\n            temp_images = os.listdir(temp_path)\n            self.class_names[c] = c_name\n\n            for i in temp_images:\n                img_tmp = os.path.join(temp_path, i)\n\n\n                if img_tmp.endswith('.jpg') or img_tmp.endswith('.JPEG'):\n                   # img = image.load_img(img_tmp, target_size=(224,224))\n                    img = cv2.imread(img_tmp, 3)\n                    model_image_size = (128, 128)\n                    resized_image = cv2.resize(img, model_image_size, interpolation = cv2.INTER_CUBIC)\n                    resized_image = resized_image.astype(np.float32) / 255.0\n                    self.images.append(resized_image)\n                    self.labels.append(c)\n                    \n\n        print('Loaded {:d} images from {:s} '.format(len(self.images), self.data_path))\n\n\n\n    def num_classes(self):\n        # returns number of classes of the dataset\n        return len(self.data_classes)\n    \n    def get_dataset(self):\n        return (list(zip(self.images, self.labels)), self.class_names)\n\n    def generate(self):\n\n        datagen = ImageDataGenerator(\n            rotation_range=10, # rotation\n            width_shift_range=0.2, # horizontal shift\n            height_shift_range=0.2, # vertical shift\n            zoom_range=0.2, # zoom\n            horizontal_flip=True, # horizontal flip\n            brightness_range=[0.2,1.2]\n            ) # brightness\n\n        train_generator = datagen.flow_from_directory(\n                  directory=self.data_path,\n                  target_size=(128, 128), # resize to this size\n                  color_mode=\"rgb\", # for coloured images\n                  batch_size=32, # number of images to extract from folder for every batch\n                  #class_mode=\"binary\", # classes to predict\n                  seed=420 # to make the result reproducible\n                  )\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "039f2092ab07411e85e50d89e4ad668f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d4831a83",
    "execution_start": 1651162120284,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 405.171875
   },
   "source": "def get_vector(image_name):\n    # 1. Load the image with Pillow library\n    img = Image.open(image_name)\n    # 2. Create a PyTorch Variable with the transformed image\n    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n    # 3. Create a vector of zeros that will hold our feature vector\n    #    The 'avgpool' layer has an output size of 512\n    my_embedding = torch.zeros(512)\n    # 4. Define a function that will copy the output of a layer\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n    # 5. Attach that function to our selected layer\n    h = layer.register_forward_hook(copy_data)\n    # 6. Run the model on our transformed image\n    model(t_img)\n    # 7. Detach our copy function from the layer\n    h.remove()\n    # 8. Return the feature vector\n    return my_embedding",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d693541e40144b74a539da6f89613a8d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "47701e61",
    "execution_start": 1651162062131,
    "execution_millis": 710,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 585.171875
   },
   "source": "import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport cv2 \n\n\n# Load the pretrained model\nmodel = models.resnet18(pretrained=True)\n\n\n# Use the model object to select the desired layer\nlayer = model._modules.get('avgpool')\n\n\n# Set model to evaluation mode\nmodel.eval()\n\n\nscaler = transforms.Resize(224)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\n\n\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "900981b7ffe2418cbdd0391cc625d0a5",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4e6a3b95",
    "execution_start": 1651164242355,
    "execution_millis": 163,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 144.4375
   },
   "source": "",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'summary'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1186\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'summary'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9b76b6e11a3445cb89112d49b8764d73",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e3d1df89",
    "execution_start": 1651162066229,
    "execution_millis": 942,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 288.453125
   },
   "source": "pic_one_vector = get_vector('/work/dataset/training/n01443537(goldfish)/n01443537_483.JPEG')\npic_two_vector = get_vector('/work/dataset/training/n01443537(goldfish)/n01443537_640.JPEG')\n\n\n# Using PyTorch Cosine Similarity\ncos = nn.CosineSimilarity(dim=1, eps=1e-6)\ncos_sim = cos(pic_one_vector.unsqueeze(0),\n              pic_two_vector.unsqueeze(0))\nprint('\\nCosine similarity: {0}\\n'.format(cos_sim))",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "output with shape [512] doesn't match the broadcast shape [1, 512, 1, 512]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-61829cea3641>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpic_one_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/work/dataset/training/n01443537(goldfish)/n01443537_483.JPEG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpic_two_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/work/dataset/training/n01443537(goldfish)/n01443537_640.JPEG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Using PyTorch Cosine Similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a45affe9d8f7>\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(image_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 6. Run the model on our transformed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 7. Detach our copy function from the layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a45affe9d8f7>\u001b[0m in \u001b[0;36mcopy_data\u001b[0;34m(m, i, o)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 4. Define a function that will copy the output of a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmy_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 5. Attach that function to our selected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [512] doesn't match the broadcast shape [1, 512, 1, 512]"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=93aceac2-8452-469e-8b02-c16d0438aa9c' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "afb5bf35-9b29-4a93-8671-172baae0b5ad",
  "deepnote_execution_queue": []
 }
}