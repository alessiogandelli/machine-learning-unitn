{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIYpbX7piBcs"
      },
      "source": [
        "# Preworkout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_64b9JgWdlwo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import vgg16\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xaxyyu8udo8c",
        "outputId": "78ccf4b8-8b52-4cb6-c77f-eb6f7b4eade1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XnZoFvrdqtR"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/datasets/unbalanced_dataset_2304.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnQxw8O8iJZv"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the VGG16 model, pre-trained on imagenet.\n",
        "\n",
        "VGG16 is a deep network with 13 convolutional layers. It was previously trained on millions of images, and has over 100,000,000 weights and biases, the majority of which connect to the first fully-connected layer (fc1). \n",
        "\n",
        "VGG-16 is setup to take a fixed-size (224 x 224 x 3) RGB image at its input, and then forward it through a series of altrnating convolutional and max-pooling layers, then capped off by three fully-connected layers of 4096, 4096, and 1000 neurons, where the last layer is our softmax classification layer.\n",
        "\n",
        "Notice that the output shape at each layer has `None` the first dimension. This is because the network can process multiple images in a single batch. So if you forward 5 images at shape [5, 224, 224, 3], then the output shape at each layer will be 5 in the first dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLmPti67eKAu",
        "outputId": "34d4d116-ffcd-4670-e9ab-47afb1f55a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 3s 0us/step\n",
            "553476096/553467096 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = vgg16.VGG16(weights='imagenet', include_top=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature extraction\n",
        "\n",
        "What we have in the model variable is a highly effective image classifier trained on the ImageNet database. We expect that the classifier must form a very effective representation of the image in order to be able to classify it with such high accuracy. We can use this to our advantage by re-purposing this for our image retrieval task. Therefore we copy the model, but remove the last layer (the classification layer), so that the final layer of the new network, called feat_extractor is the second 4096-neuron fully-connected layer, \"fc2 (Dense)\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tknv8HKseZ76"
      },
      "outputs": [],
      "source": [
        "feat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFu7I0aJiMTG"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOlYqss5iWHz"
      },
      "source": [
        "## test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create the variables query_images and gallery_images, which are lists of paths of the various pictures.\n",
        "Then, we will begin a loop which will open each image, extract its feature vector, and append it to two lists called query_features and gallery_features which will contain our activations for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc3JsIjsdYC9",
        "outputId": "f76d40a4-6932-4275-dd43-b078905495ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 8777 images from /content/unbalanced_dataset_2304/validation/gallery/ \n",
            "Loaded 2236 images from /content/unbalanced_dataset_2304/validation/query/ \n"
          ]
        }
      ],
      "source": [
        "import utils\n",
        "GALLERY_PATH = \"/content/gallery/\"\n",
        "QUERY_PATH = \"/content/query/\"\n",
        "\n",
        "target_shape = (224, 224)\n",
        "\n",
        "gallery = utils.Dataset(GALLERY_PATH, target_shape = target_shape).get_dataset()\n",
        "query = utils.Dataset(QUERY_PATH, target_shape = target_shape).get_dataset()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxD4CL45iZEV"
      },
      "source": [
        "## extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Now we use our feature extractor on our competition data. \n",
        "The compute_features function returns an array with one element per image. Each element contains a 4096-element array, \n",
        "which is the activations of the last fully-connected layer fc2 in VGG16.\n",
        "\n",
        "We expect that the fc2 activations form a very good representation of the image, such that similar images should \n",
        "produce similar activations. In other words, the fc2 activations of two images which have similar content should \n",
        "be very close to each other. We can exploit this to do information retrieval. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C_0gLUxelkt"
      },
      "outputs": [],
      "source": [
        "query_features, query_urls, query_labels = utils.compute_features(query, feat_extractor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h5Dtn0PhGtr"
      },
      "outputs": [],
      "source": [
        "gallery_features, gallery_urls, gallery_labels = utils.compute_features(gallery, feat_extractor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## without PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to do our reverse image queries. Both query_features and gallery_features contain a compact representation of our images, one 4096-element row for each image. The assumption we can now make is that two images which have similar content, should produce similar feature vectors.\n",
        "In order to do image retrieval, though, we first need to decide a measurement of the distance between each query feature vector and all the gallery ones. We choose to use both euclidean distance and cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_no_PCA_eu = utils.compute_results(query_features, gallery_features, query_urls, gallery_urls)\n",
        "results_no_PCA_cos = utils.compute_results(query_features, gallery_features, query_urls, gallery_urls, dist= 'cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## with PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, we are done with our image retrieval task. Nonetheless, we want to try also to do something more on top of this.\n",
        "In particular, we would like to apply a PCA algorithm on the 4096-element feature vector to try to reduce the dimensionality of our feature vectors down to 30. This is for two reasons: \n",
        "1) the 4096 feature vector may have some redundancy in it, such that multiple elements in the vector are highly correlated or similar. This would skew similarity comparisons towards those over-represented features. \n",
        "2) Operating over 4096 elements is inefficient both in terms of space/memory requirements and processor speed, and it would be better for us if we can reduce the length of these vectors but maintain the same effective representation.\n",
        "3) We are also interested in seeing how much our retrieval performance decreases when we go from a 4096-element vector to a 30-element one. As a matter of fact, we are decreasing our vector size by a factor of 128!\n",
        "\n",
        "The next cell will instantiate a PCA object, which we will then fit our data to, choosing to keep the top 30 principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca = PCA(n_components=30)\n",
        "pca.fit(query_features)\n",
        "\n",
        "pca = PCA(n_components=30)\n",
        "pca.fit(gallery_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pca object stores the actual transformation matrix which was fit in the previous cell. We can now use it to transform any original feature vector (of length 4096) into a reduced 30-dimensional feature vector in the principal component space found by the PCA. \n",
        "\n",
        "So we take our original feature vectors, and transform them to the new space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_pca_features = pca.transform(query_features)\n",
        "gallery_pca_features = pca.transform(gallery_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_PCA = utils.compute_results(query_pca_features, gallery_pca_features, query_urls, gallery_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# competition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def submit(results, url=\"http://tinyurl.com/IML2022\"):\n",
        "    res = json.dumps(results)\n",
        "    response = requests.post(url, res)\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        print(f\"accuracy is {result['results']}\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"ERROR: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "mydata = dict()\n",
        "mydata['groupname'] = \"The Ostriches\"\n",
        "\n",
        "mydata[\"images\"] = results_PCA\n",
        "submit(mydata)\n",
        "\n",
        "mydata[\"images\"] = results_no_PCA_eu\n",
        "submit(mydata)\n",
        "\n",
        "mydata[\"images\"] = results_no_PCA_cos\n",
        "submit(mydata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suH2dIvmib9h"
      },
      "source": [
        "## compute results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPh2jtlVh8rB",
        "outputId": "6a8c3fe8-27c6-491a-cf1d-3dd6631e33d8"
      },
      "outputs": [],
      "source": [
        "utils.evaluate(results_PCA, query_labels, gallery_labels)\n",
        "utils.evaluate(results_no_PCA_eu, query_labels, gallery_labels)\n",
        "utils.evaluate(results_no_PCA_cos, query_labels, gallery_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Ale_VGG_PCA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
